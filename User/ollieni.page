---
title: ollieni (倪英智)
categories: User
...

## 2024 Linux 核心設計 春季班 自我評量

* 簡介：國立成功大學 敏求智慧運算學院 人工智慧機器人 114 級
* GitHub: [ollieni](https://github.com/ollieni)
* HackMD: [NIYINGCHIH](https://hackmd.io/@NIYINGCHIH)

### 成果發表與貢獻
課程教材修訂

* [你所不知道的 C 語言：編譯器和最佳化原理篇](https://hackmd.io/@sysprog/c-compiler-optimization): Braif*ck -> Brainf*ck、.改成，、增加空白、增加程式的格式

* [你所不知道的 C 語言：編譯器原理和案例分析](https://hackmd.io/@sysprog/c-compiler-construction): 又 -> 有

* [錯誤更正碼介紹和實作考量](https://hackmd.io/@sysprog/r1wrjOp6a): 增加空白，漢明離 -> 漢明距離

* [你所不知道的 C 語言: 執行階段程式庫 (CRT)](https://hackmd.io/@sysprog/c-runtime): imgur 圖片無法顯示修正

* [你所不知道的 C 語言：連結器和執行檔資訊](https://hackmd.io/@sysprog/c-linker-loader): RELA relocation -> RELA relocations

* [你所不知道的 C 語言：動態連結器篇](https://hackmd.io/@sysprog/c-dynamic-linkage): 作中學 -> 做中學

* [你所不知道的 C 語言：物件導向程式設計篇](https://hackmd.io/@sysprog/c-oop): develeopment -> development

### 作業與隨堂測驗
* lab0: [HackMD](https://hackmd.io/@NIYINGCHIH/linux2024-homework1)
完成指定的佇列操作，針對使用到的 git hook 和 clang-format 做查詢並理解，完成 list_sort 和 Tim_sort 的引入，實作 Fisher–Yates shuffle。

* quiz1 + 2: [HackMD](https://hackmd.io/@NIYINGCHIH/2024q1_Homework2)


* quiz3 + 4: [HackMD](https://hackmd.io/@NIYINGCHIH/linux2024-homework4)

* Assessment: [HackMD](https://hackmd.io/@NIYINGCHIH/2024q1_Homework5_assessment)
撰寫閱讀〈因為自動飲料機而延畢的那一年〉後的心得，研讀課程教材並提出疑問。
* Integration: [HackMD](https://hackmd.io/@NIYINGCHIH/linux2024-homework6)

### 期末專題

Pub/Sub模擬: [HackMD](https://hackmd.io/@NIYINGCHIH/PubSub)

### 與授課教師的互動
一對一討論：5月 20日 (星期一)⋅下午 7:00 - 7:30

* bi-endianness 如何切換?

會在開機時做選擇，若要在開機後做轉換會很麻煩。

* CUDA 如何串接軟硬體，讓程式可以運用 GPU 資源?

授課教師告知我還有 OpenCL 這個框架，CUDA 和 OpenCL 都是平行運算框架，OpenCL可以支援很多種計算設備，如 CPU、GPU、FPGA，而 CUDA 只能支援 Nvidia 的 GPU。

* 6/20 上課 按計算機計算答案

### 修課心得

### 與指導教授的學習回顧
二月份回顧 : 3月7日 週四 下午 10:41

* 指標的操作、bitwise-operation、<list.h> 的運用，以及工程師的好素養 : 怎麼樹立一致且易於協作的程式開發規範、如何使用 Git 以及寫出好的 commit message，研究自動測試機制。

三月份回顧 : 4月8日 週一 下午 8:32

* 陸游的名言："紙上得來終覺淺，絕知此事要躬行"，以及蔡志浩的觀點："大部分的人一輩子洞察力不彰，原因之一是怕講錯被笑。想了一點點就不敢繼續也沒記錄或分享，時間都花在讀書查資料看別人怎麼想。看完就真的沒有自己的洞察了"

* <The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits>

### 自我評量

